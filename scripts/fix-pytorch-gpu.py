#!/usr/bin/env python3
"""
Fix PyTorch GPU Installation
Upgrades from CPU-only to GPU-accelerated PyTorch for fast embeddings
"""

import sys
import subprocess
import importlib.util

def check_current_pytorch():
    """Check current PyTorch installation"""
    print("üîç Checking current PyTorch installation...")
    
    try:
        import torch
        print(f"   Current PyTorch version: {torch.__version__}")
        print(f"   CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"   CUDA version: {torch.version.cuda}")
            print(f"   GPU device count: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
        else:
            print("   ‚ùå CUDA not available - CPU-only installation detected!")
        return torch.cuda.is_available()
    except ImportError:
        print("   ‚ùå PyTorch not installed")
        return False

def install_gpu_pytorch():
    """Install GPU-accelerated PyTorch"""
    print("\nüîß Installing GPU-accelerated PyTorch...")
    
    # CUDA 12.2 is compatible with PyTorch CUDA 12.1 wheels
    commands = [
        # Uninstall CPU version first
        [sys.executable, "-m", "pip", "uninstall", "torch", "torchvision", "torchaudio", "-y"],
        
        # Install GPU version with CUDA 12.1 (compatible with CUDA 12.2)
        [sys.executable, "-m", "pip", "install", "torch", "torchvision", "torchaudio", 
         "--index-url", "https://download.pytorch.org/whl/cu121"],
    ]
    
    for i, cmd in enumerate(commands, 1):
        print(f"\n   Step {i}: {' '.join(cmd[3:])}")
        try:
            result = subprocess.run(cmd, check=True, capture_output=True, text=True)
            print(f"   ‚úÖ Success")
        except subprocess.CalledProcessError as e:
            print(f"   ‚ùå Error: {e}")
            print(f"   stdout: {e.stdout}")
            print(f"   stderr: {e.stderr}")
            return False
    
    return True

def verify_gpu_installation():
    """Verify GPU PyTorch is working"""
    print("\n‚úÖ Verifying GPU PyTorch installation...")
    
    try:
        # Force reload torch module
        if 'torch' in sys.modules:
            del sys.modules['torch']
        
        import torch
        print(f"   PyTorch version: {torch.__version__}")
        print(f"   CUDA available: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"   CUDA version: {torch.version.cuda}")
            print(f"   GPU count: {torch.cuda.device_count()}")
            
            # Test GPU tensor creation
            device = torch.device('cuda')
            test_tensor = torch.randn(1000, 1000, device=device)
            print(f"   GPU tensor test: ‚úÖ Success")
            print(f"   GPU device: {torch.cuda.get_device_name(0)}")
            print(f"   GPU memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB")
            
            return True
        else:
            print("   ‚ùå CUDA still not available after installation")
            return False
            
    except Exception as e:
        print(f"   ‚ùå Error during verification: {e}")
        return False

def benchmark_performance():
    """Benchmark GPU vs CPU performance"""
    print("\nüìä Benchmarking GPU performance...")
    
    try:
        import torch
        import time
        
        # Test tensor size for embedding simulation
        size = (512, 768)  # Typical embedding dimensions
        
        # CPU benchmark
        print("   Testing CPU performance...")
        torch.backends.cudnn.benchmark = True
        cpu_device = torch.device('cpu')
        start_time = time.time()
        
        for _ in range(10):
            a = torch.randn(*size, device=cpu_device)
            b = torch.randn(*size, device=cpu_device)
            c = torch.mm(a, b.t())
        
        cpu_time = time.time() - start_time
        print(f"   CPU time (10 iterations): {cpu_time:.3f}s")
        
        # GPU benchmark
        if torch.cuda.is_available():
            print("   Testing GPU performance...")
            gpu_device = torch.device('cuda')
            torch.cuda.synchronize()
            start_time = time.time()
            
            for _ in range(10):
                a = torch.randn(*size, device=gpu_device)
                b = torch.randn(*size, device=gpu_device)
                c = torch.mm(a, b.t())
            
            torch.cuda.synchronize()
            gpu_time = time.time() - start_time
            print(f"   GPU time (10 iterations): {gpu_time:.3f}s")
            print(f"   üöÄ Speedup: {cpu_time/gpu_time:.1f}x faster")
        
    except Exception as e:
        print(f"   ‚ùå Benchmark error: {e}")

def main():
    print("üéØ PYTORCH GPU INSTALLATION FIX")
    print("=" * 50)
    print("This script fixes slow embeddings by installing GPU-accelerated PyTorch\n")
    
    # Check current installation
    has_gpu = check_current_pytorch()
    
    if has_gpu:
        print("\n‚úÖ GPU PyTorch already installed!")
        benchmark_performance()
        return
    
    print("\nüö® ISSUE DETECTED: CPU-only PyTorch installation")
    print("   This causes slow embeddings (10-30s instead of <1s)")
    print("   GPU detected: NVIDIA RTX 4060 with CUDA 12.2")
    
    # Install GPU version
    if install_gpu_pytorch():
        print("\nüîÑ Restarting Python to load new PyTorch...")
        # In a real deployment, you'd restart the process here
        
        if verify_gpu_installation():
            print("\nüéâ GPU PyTorch installation successful!")
            benchmark_performance()
        else:
            print("\n‚ùå GPU installation failed - check CUDA drivers")
    else:
        print("\n‚ùå Failed to install GPU PyTorch")

if __name__ == "__main__":
    main()